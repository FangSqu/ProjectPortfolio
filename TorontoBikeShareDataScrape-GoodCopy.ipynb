{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4686859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\benhg\\anaconda3\\lib\\site-packages (4.16.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\benhg\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e82c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "        \n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2df14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "# Run Chrome in headless mode (without opening a browser window)\n",
    "\n",
    "# Create a WebDriver instance\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Load the page\n",
    "URL = \"https://bikesharetoronto.com/?_gl=1%2A8bk15e%2A_ga%2AMjAxOTE5MTA0LjE3MDE5MDUwODk.%2A_ga_QSHG2WN95L%2AMTcwMTkwNTA4OS4xLjEuMTcwMTkwNTIyNC4wLjAuMA..\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Wait for some time (or use WebDriverWait) to let dynamic content load\n",
    "# ...\n",
    "\n",
    "# Get the page source after dynamic content has loaded\n",
    "page_source = driver.page_source\n",
    "\n",
    "#Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Now, create a BeautifulSoup object as before\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15c20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(soup.prettify(),\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "# Create a WebDriver instance\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Load the page\n",
    "URL = \"https://bikesharetoronto.com/?_gl=1%2A8bk15e%2A_ga%2AMjAxOTE5MTA0LjE3MDE5MDUwODk.%2A_ga_QSHG2WN95L%2AMTcwMTkwNTA4OS4xLjEuMTcwMTkwNTIyNC4wLjAuMA..\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Get the page source after dynamic content has loaded\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Get current time\n",
    "time_min = time.strftime(\"%y-%m-%d %H:%M\")\n",
    "\n",
    "# All Station info\n",
    "\n",
    "# HOME\n",
    "HOME_bikes = soup.find(id='7126')\n",
    "HOME_bikes_ava = HOME_bikes.find('div', class_='infotxt').text.strip() if HOME_bikes else \"Not Available\"\n",
    "HOME_num_docks = 17\n",
    "\n",
    "# Wilcocks\n",
    "AC_Wil_bikes = soup.find(id='7170')\n",
    "AC_Wil_bikes_ava = AC_Wil_bikes.find('div', class_='infotxt').text.strip() if AC_Wil_bikes else \"Not Available\"\n",
    "AC_Wil_num_docks = 15\n",
    "\n",
    "# Harbour\n",
    "AC_Har_bikes = soup.find(id='7285')\n",
    "AC_Har_bikes_ava = AC_Har_bikes.find('div', class_='infotxt').text.strip() if AC_Har_bikes else \"Not Available\"\n",
    "AC_Har_num_docks = 36\n",
    "\n",
    "# Robarts\n",
    "AC_Rob_bikes = soup.find(id='7058')\n",
    "if AC_Rob_bikes:\n",
    "    AC_Rob_bikes_ava = AC_Rob_bikes.find('div', class_='infotxt').text.strip()\n",
    "    print(AC_Rob_bikes_ava)\n",
    "else:\n",
    "    print(\"Station not found\")\n",
    "AC_Rob_bikes_ava.strip()[49:]\n",
    "\n",
    "AC_Rob_num_docks = 19\n",
    "\n",
    "# Define data for each station\n",
    "stations_data = [\n",
    "    ('HOME', time_min, HOME_bikes_ava.strip()[17:], HOME_num_docks),\n",
    "    ('AC_Wil', time_min, AC_Wil_bikes_ava.strip()[17:], AC_Wil_num_docks),\n",
    "    ('AC_Har', time_min, AC_Har_bikes_ava.strip()[17:], AC_Har_num_docks),\n",
    "    ('Rob', time_min, AC_Rob_bikes_ava.strip()[17:], AC_Rob_num_docks)\n",
    "]\n",
    "\n",
    "# Write data to CSV\n",
    "with open('BikeTrack_HOME.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Station_Name', 'Time', 'Bikes_ava', 'Total_Docks'])\n",
    "    for data in stations_data:\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4d3dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available bikes: 1\n"
     ]
    }
   ],
   "source": [
    "def check_bikes_new():\n",
    "    \n",
    "    \n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import csv\n",
    "    import time\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "# Create a WebDriver instance\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Load the page\n",
    "URL = \"https://bikesharetoronto.com/?_gl=1%2A8bk15e%2A_ga%2AMjAxOTE5MTA0LjE3MDE5MDUwODk.%2A_ga_QSHG2WN95L%2AMTcwMTkwNTA4OS4xLjEuMTcwMTkwNTIyNC4wLjAuMA..\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Get the page source after dynamic content has loaded\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Get current time\n",
    "time_min = time.strftime(\"%y-%m-%d %H:%M\")\n",
    "\n",
    "# All Station info\n",
    "\n",
    "# HOME\n",
    "HOME_bikes = soup.find(id='7126')\n",
    "HOME_bikes_ava = HOME_bikes.find('div', class_='infotxt').text.strip() if HOME_bikes else \"Not Available\"\n",
    "HOME_num_docks = 17\n",
    "\n",
    "# Wilcocks\n",
    "AC_Wil_bikes = soup.find(id='7170')\n",
    "AC_Wil_bikes_ava = AC_Wil_bikes.find('div', class_='infotxt').text.strip() if AC_Wil_bikes else \"Not Available\"\n",
    "AC_Wil_num_docks = 15\n",
    "\n",
    "# Harbour\n",
    "AC_Har_bikes = soup.find(id='7285')\n",
    "AC_Har_bikes_ava = AC_Har_bikes.find('div', class_='infotxt').text.strip() if AC_Har_bikes else \"Not Available\"\n",
    "AC_Har_num_docks = 36\n",
    "\n",
    "# Robarts\n",
    "AC_Rob_bikes = soup.find(id='7058')\n",
    "if AC_Rob_bikes:\n",
    "    AC_Rob_bikes_ava = AC_Rob_bikes.find('div', class_='infotxt').text.strip()\n",
    "    print(AC_Rob_bikes_ava)\n",
    "else:\n",
    "    print(\"Station not found\")\n",
    "AC_Rob_bikes_ava.strip()[49:]\n",
    "\n",
    "AC_Rob_num_docks = 19\n",
    "\n",
    "# Define data for each station\n",
    "stations_data = [\n",
    "    ('HOME', time_min, HOME_bikes_ava.strip()[17:], HOME_num_docks),\n",
    "    ('AC_Wil', time_min, AC_Wil_bikes_ava.strip()[17:], AC_Wil_num_docks),\n",
    "    ('AC_Har', time_min, AC_Har_bikes_ava.strip()[17:], AC_Har_num_docks),\n",
    "    ('Rob', time_min, AC_Rob_bikes_ava.strip()[17:], AC_Rob_num_docks)\n",
    "]\n",
    "\n",
    "# Write data to CSV\n",
    "with open('BikeTrack_HOME.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Station_Name', 'Time', 'Bikes_ava', 'Total_Docks'])\n",
    "    for data in stations_data:\n",
    "        writer.writerow(data)\n",
    "        \n",
    "# ^ this part is what makes is more organized is csv compare to Virsion 1\n",
    "   #Compare:\n",
    "    ###  \"with open('BikeTrack_HOME.csv', 'w', newline = '', encoding = 'UTF8') as f:\n",
    "        #writer = csv.writer(f)\n",
    "       # writer.writerow(header)\n",
    "       # writer.writerow(data)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf036ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# append every 30 mins\n",
    "# 60*30 = 1800 seconds\n",
    "\n",
    "while True: \n",
    "    check_bikes_new()\n",
    "    time.sleep(1800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a217a93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Station_Name            Time  Bikes_ava  Total_Docks\n",
      "0           HOME  24-03-09 09:45        NaN           17\n",
      "1         AC_Wil  24-03-09 09:45        NaN           15\n",
      "2         AC_Har  24-03-09 09:45        NaN           36\n",
      "3            Rob  24-03-09 09:45        NaN           19\n",
      "4   Station_Name            Time  Bikes_ava  Total_Docks\n",
      "5           HOME  24-03-09 09:58        NaN           17\n",
      "6         AC_Wil  24-03-09 09:58        NaN           15\n",
      "7         AC_Har  24-03-09 09:58        NaN           36\n",
      "8            Rob  24-03-09 09:58        NaN           19\n",
      "9   Station_Name            Time  Bikes_ava  Total_Docks\n",
      "10          HOME  24-03-09 10:07          1           17\n",
      "11        AC_Wil  24-03-09 10:07          0           15\n",
      "12        AC_Har  24-03-09 10:07          5           36\n",
      "13           Rob  24-03-09 10:07          1           19\n",
      "14  Station_Name            Time  Bikes_ava  Total_Docks\n",
      "15          HOME  24-03-09 10:09          1           17\n",
      "16        AC_Wil  24-03-09 10:09          0           15\n",
      "17        AC_Har  24-03-09 10:09          6           36\n",
      "18           Rob  24-03-09 10:09          1           19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\benhg\\BikeTrack_HOME.csv')\n",
    "\n",
    "print (df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
